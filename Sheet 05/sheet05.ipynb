{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "451ad666",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bd1a7235f844ff174a795a6efe33a78",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2021) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Axel Schaffland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25658cd4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f01b02cef2683e03ee455e98b30f6f5c",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 05: Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487bc951",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d3f1c7b38dfd69fe6f5525380c408e9",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before the end of **Tuesday, May 25, 2021**. If you need help (and Google and other resources were not enough), feel free to contact your groups designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86800736",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20ee1eaee75301849f3c518d43a914f2",
     "grade": false,
     "grade_id": "cell-0ceaa7378e4a713d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 0: Math recap (Derivatives in higher dimensions) [0 Points]\n",
    "\n",
    "This exercise is supposed to be very easy but in this and the following sheets we will give points. There will be a similar exercise on every sheet. It is intended to revise some basic mathematical notions that are assumed throughout this class and to allow you to check if you are comfortable with them. Usually you should have no problem to answer these questions offhand, but if you feel unsure, this is a good time to look them up again. You are always welcome to discuss questions with the tutors or in the practice session. Also, if you have a (math) topic you would like to recap, please let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a85779",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6eb9238f1420f5fae66c47669e8d567d",
     "grade": false,
     "grade_id": "cell-106b918b6f9c6fea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What is a partial derivative? What is a directional derivative? How are these computed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db65f54",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36475d2d0f85070bc2f5c7617633e228",
     "grade": true,
     "grade_id": "cell-f80e2cfbc5dae96a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dab020",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "33165181f1f536046a89dea273b1dd3c",
     "grade": false,
     "grade_id": "cell-10c6f038150609e1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** What is the gradient, the Jacobian matrix, and the Hessian matrix? How are they computed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eac448",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4ff3d7e15d56e4dbc54eaef465500577",
     "grade": true,
     "grade_id": "cell-c45db6ae30a5507a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd20d15",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ce944e185f8cdab12815c71da691493",
     "grade": false,
     "grade_id": "cell-7822385798587c45",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** What is the chain rule (in calculus)? How does it look in the higher-dimensional case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c106a3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "94696dd22018d912b2a84a73c0a13030",
     "grade": true,
     "grade_id": "cell-1a5e17baf68e02e1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    },
    "solution": true
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3627f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5fff736d22c11bf3b5c3a1ff705c2c43",
     "grade": false,
     "grade_id": "ex1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Curse of Dimensionality [5 Points]\n",
    "\n",
    "For the following exercise, be detailed in your answers and provide some examples. Think about keywords like: random vectors in high dimensional space, manifolds and Bertillonage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de8a840",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "edb370f033b49f58a5eb149c48dd3201",
     "grade": false,
     "grade_id": "ex1_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**a)** What are the curse of dimensionality and its implication for pattern classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bdd7a5",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d26d8a53a81b62acb56efb18efe45591",
     "grade": true,
     "grade_id": "ex1_a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0fbb92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3ea2c6702a7f9c037c56ec86e64f9da",
     "grade": false,
     "grade_id": "ex1_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**b)** Explain how this phenomenom could be used to one's advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e8eb7",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f8b9810e6a1f28473be5497241413e7b",
     "grade": true,
     "grade_id": "ex1_b_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba5693d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "131868a391e6695a8f1273960e3dc171",
     "grade": false,
     "grade_id": "ex1_c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**c)** Explain in your own words the concepts of descriptive and intrinsic dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d59d36",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e9bae1c3fb4af5de4cba4f757e00584",
     "grade": true,
     "grade_id": "ex1_c_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac6d9a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7746eba43118373283840b64631343a0",
     "grade": false,
     "grade_id": "cell-4b2a7291c0d81d1a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**d)** The $n$-dimensional unit cube: A cube with edge length $d=1$ in the $n$-dimensional space $\\mathbb{R}^n$. Compute the volume and the length of the diagonal for $n = 1, 2, 3, 4, 5, 10, 100, 1000$. Do the same for $d=\\frac{1}{2}$. Discuss the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9475fc4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58694a7912a5df0a4741953a1013ebfc",
     "grade": true,
     "grade_id": "cell-70999ba1b1b35036",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba785b2a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "949f02f5f8e7a6f31b80900a3782c888",
     "grade": true,
     "grade_id": "cell-16d9afe9225178e8",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a053de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fa4c13c725e6a70b690a33dd6490058",
     "grade": false,
     "grade_id": "cell-f981fd573a23eed2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "**e)** The $n$-dimensional unit ball: A ball with radius $r=1$ in the $n$-dimensional space $\\mathbb{R}^n$. Compute the volume for $n = 1,2,3,5,10,100,300,400$. Use the following formula to compute the volume $V$ for a given dimension $n$ and radius $R$:\n",
    "\n",
    "$$ V_{n}(R)\\sim {\\frac {1}{\\sqrt {n\\pi }}}\\left({\\frac {2\\pi e}{n}}\\right)^{\\frac {n}{2}}R^{n}$$\n",
    "\n",
    "Refer to https://en.wikipedia.org/wiki/Volume_of_an_n-ball for more details.\n",
    "\n",
    "Consider a $n$-dimensional unit orange consisting of the peel and the pulp: Let the thickness of the peel be $1\\%$ of the radius. Compute the volume of the peel and compare to the volume of the whole orange for the same values of $n$. State the implications of your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868eef7b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "662e42fb811b8fe6f900a68798fbd134",
     "grade": true,
     "grade_id": "cell-8d659c961b351eac",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8719f5",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a9d6857bb4857c7d9d68384be570bb2",
     "grade": true,
     "grade_id": "cell-bee514604d4d11a9",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34edf84e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf3ad2839327c17fb69f8d7eeedb23c6",
     "grade": false,
     "grade_id": "ex2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 2: Implement and Apply PCA [8 Points]\n",
    "\n",
    "In this assignment you will implement PCA from the ground up and apply it to the `cars` dataset (simplified from the JSE [2004 New Car and Truck Data](http://www.amstat.org/publications/jse/jse_data_archive.htm)). This dataset consists of measurements taken on 97 different cars. The eleven features measured are: Suggested retail price (USD), Price to dealer (USD), Engine size (liters), Number of engine cylinders, Engine horsepower, City gas mileage, Highway gas mileage, Weight (pounds), Wheelbase (inches), Length (inches) and Width (inches).\n",
    "\n",
    "We would like to visualize these high dimensional features to get a feeling for how the cars relate to each other so we need to find a subspace of dimension two or three into which we can project the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261fa6b0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "325114529c4799bcf74afdf8403f3b99",
     "grade": true,
     "grade_id": "ex2_a_solution",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: Load the cars dataset in cars.csv .\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert cars.shape == (97, 11), \"Shape is not (97, 11), was {}\".format(cars.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e4088",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d4f4d4287acb712e068a0cf6f5e7c32",
     "grade": false,
     "grade_id": "cell-f34c576eec4a5968",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Excecute the following code which will create a scatter plot matrix (it might take some time to execute). This should give you an idea about trends and correlations in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43db5836",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "638d8d78d53022d460e00af541e220e3",
     "grade": false,
     "grade_id": "cell-dea96f209e45dc32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "cols = ['Suggested retail price (USD)', 'Price to dealer (USD)',\n",
    "          'Engine size (liters)', 'Number of engine cylinders',\n",
    "          'Engine horsepower', 'City gas mileage' ,\n",
    "          'Highway gas mileage', 'Weight (pounds)',\n",
    "          'Wheelbase (inches)', 'Length (inches)', 'Width (inches)']\n",
    "\n",
    "df = pd.DataFrame(cars, columns=cols)\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53473fd3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bb8a0b010e2299c3490f89df7bc1711",
     "grade": false,
     "grade_id": "cell-b2aebe0aabf8db03",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "What is the covariance matrix of a dataset? What do the different entries of the matrix mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edd267f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e38327010723cbbf66fd5dc57ad3d4e",
     "grade": true,
     "grade_id": "cell-bd9c31f2e24eac43",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32feefd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a280b18e2eb0e303544ab1a18443e81",
     "grade": false,
     "grade_id": "ex2_c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "PCA finds a subspace that maximizes the variance by determining the eigenvectors of the covariance matrix. So we need to calculate the covariance matrix and afterwards the eigenvalues. When the data is normalized the covariance is calculated as\n",
    "\n",
    "$$C = \\frac{1}{n-1}((\\bf{X}-\\bar{x})^T(\\bf{X}-\\bar{x})) $$\n",
    "\n",
    "with $X$ being an $n \\times d$ matrix with $n$ samples and $d$ features, when $\\bar{X}$ is the mean vector of features\n",
    "$$\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}x_i.$$  \n",
    "\n",
    "The entry $c_{i,j}$ in $C$ (a $d\\times d$ matrix) tells you how much feature $i$ correlates with feature $j$.\n",
    "\n",
    "\n",
    "**Note**: When the features have different scales, for achieving comparable covariance values, **first and before calculating the covariance matrix**, we need to standardize $X$ respecting its features\n",
    "\n",
    "$${\\bf{X}}_{norm} = \\frac{\\bf{X}}{\\sigma}$$\n",
    "\n",
    "when $\\sigma$ is the standard deviation vector of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5790fe3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "19ffe03a10a5411ff63b7c91046f0326",
     "grade": true,
     "grade_id": "ex2_c_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Compute the covariance matrix and store it into covar\n",
    "# YOUR CODE HERE\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert covar.shape == (11, 11)\n",
    "\n",
    "# TODO: Compute the eigenvalues and eigenvectors and store them into eigenval and eigenvec\n",
    "#       (Figure out a function to do this for you)\n",
    "# YOUR CODE HERE\n",
    "# YOUR CODE HERE\n",
    "\n",
    "assert eigenval.shape == (11,)\n",
    "assert eigenvec.shape == (11, 11)\n",
    "for ev in eigenvec: np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027a328f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a2a471db1d7416f070a272c89e9be4ae",
     "grade": false,
     "grade_id": "cell-3573629c4379a229",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Plot the spectrum of the eigenvalues and make sure that they are sorted by their magnitude (in descending order).\n",
    "\n",
    "**Note:** Sorting should be done respecting eigenvalues vector, but the order of eigenvectors should also be updated such that the corresponding pairs of eigenvalue-eigenvector be accessible with the same index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c00e07",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97530867fb2b77ecab4f3d13f8822a04",
     "grade": true,
     "grade_id": "cell-53f8a29f237fc60e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481857f5",
   "metadata": {},
   "source": [
    "How many principal components should you include based on the spectrum plot?\n",
    "\n",
    "One method to decide about the number of components is the \"explained variance.\" The amount of data variance captured by each principal components is the magnitude of its corresponding eigenvalue. Therefore, in the explained variance method, we calculate the proportion of each the eigenvalue respective to the total sum of the eigenvalues. That gives us the percentage of data variance explained by each corresponding principal component. And the cumulative sum of these percentages shows how much more of the dataset information (variance) is presentable with taking one more component. Knowing that on one hand, and the computational cost and the difficulty of visualization of one more dimension gives a clue for the decision.\n",
    "\n",
    "Execute the cells below and decide about an efficient number of principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_exp = [val*100/sum(eigenval) for val in eigenval]\n",
    "cum_sum = np.cumsum(var_exp)\n",
    "\n",
    "plt.bar(range(len(eigenval)), var_exp, label='Explained Variance')\n",
    "plt.step(range(len(eigenval)), cum_sum, 'r:',\n",
    "         where='mid', label='Cumulative Sum')\n",
    "plt.ylabel('Explained Variance (%)')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc='center right');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d183c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e4f738fcd837a12810c5950e3dde8f8",
     "grade": false,
     "grade_id": "ex2_d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "As you can see, with the first eigenvalue/component we can describe about 70% of the variance in our dataset; taking the first two will increase it to more than 80%. For the sake of a convinient visualization we will go with the first two components, because the increase of the variation is not that much after the second component.\n",
    "\n",
    "Now you should have a matrix full of eigenvectors. We can now do two things: project the data down onto the two dimensional subspace to visualize it and we can also plot the two first principle component vectors as eleven two dimensional points to get a feeling for how the features are projected into the subspace. Execute the two cells below and describe what you see. Is PCA a good method for this problem? Was it justifiable that we only considered the first two principle components? What kinds of cars are in the four quadrants of the second plot? (**put your answer in the cell below of this code cell**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7853be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "677e27490dcf885a28fea9784f854123",
     "grade": false,
     "grade_id": "cell-a2968be21ca0a092",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Project the data down into the two dimensional subspace\n",
    "proj = cars_norm @ eigenvec[:,:2]\n",
    "\n",
    "\n",
    "# Plot projected data\n",
    "plt.title('Data Projected Onto First two Principal Components')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('PC1 ({}%)'.format(round(var_exp[0])))\n",
    "plt.ylabel('PC2 ({}%)'.format(round(var_exp[1])))\n",
    "plt.scatter(proj[:,0], proj[:,1], alpha=.4)\n",
    "\n",
    "# Plot the PC Vectors\n",
    "# Project them and scale them by the standard deviation.\n",
    "eigenvec_p = eigenvec.T @ eigenvec * np.sqrt(eigenval)\n",
    "origin = np.mean(proj, axis=0)\n",
    "\n",
    "plt.quiver(*origin, eigenvec_p[0,0], eigenvec_p[0,1], angles='xy', scale_units='xy', scale=1)\n",
    "plt.quiver(*origin, eigenvec_p[1,0], eigenvec_p[1,1], angles='xy', scale_units='xy', scale=1)\n",
    "\n",
    "\n",
    "plt.text(-12,-17.5, 'PC1')\n",
    "plt.text(-18,-15, 'PC2');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee646d78",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b7df0ad8a97e0929dcecc85855fa066",
     "grade": false,
     "grade_id": "cell-a2968be21ca0a091",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Set the plot\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "plt.title('Eigenvectors Plot')\n",
    "plt.axhline(0, color='green', linestyle=':')\n",
    "plt.axvline(0, color='green', linestyle=':')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('PC1 ({}%)'.format(round(var_exp[0])))\n",
    "plt.ylabel('PC2 ({}%)'.format(round(var_exp[1])))\n",
    "\n",
    "\n",
    "# plot centered projected data\n",
    "proj = proj - np.mean(proj, axis=0)\n",
    "\n",
    "\n",
    "plt.scatter(proj[:,0], proj[:,1], alpha=.4)\n",
    "\n",
    "# scale eigenvectors\n",
    "eigenvec_s = eigenvec * np.sqrt(eigenval)\n",
    "\n",
    "# Plot the eigenvector and add the labels\n",
    "for idx, eivec in enumerate(eigenvec_s[:,:2]):\n",
    "    plt.arrow(0, 0, eivec[0]*5, eivec[1]*5, alpha=.8, \n",
    "              color=plt.get_cmap('Set3')(idx), \n",
    "              width=0.03, head_width=.2, label=cols[idx])   \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e7830a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e582879d8311a72a87b382a34ab12773",
     "grade": true,
     "grade_id": "ex2_d_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8381195c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e7cb692f586b44e41bad2c418f9fc73",
     "grade": false,
     "grade_id": "ex3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 3: PCA [3 Points]\n",
    "\n",
    "In this exercise we investigate the statement from the lecture that PCA finds the subspace that captures most of the data variance. To be more precise, we show that the orthonormal projection onto an $m$-dimensional subspace that maximizes the variance of the projected data is defined by the principal components, i.e. by the $m$ eigenvectors of the correlation matrix $C$ corresponding to the $m$ largest eigenvalues. The proof consists of two steps. In this exercise you will only proof the first step:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac9adb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f70335217b2de3f15110fba6dac6a2bb",
     "grade": false,
     "grade_id": "ex3_a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### First step:\n",
    "Consider a one dimensional subspace: Determine a (unit) vector $\\vec{p}$, such that the variance of the data, when projected onto the subspace determined by that vector, is maximal.\n",
    "\n",
    "The correlation matrix $C$ allows to compute the variance of the projected data as $\\vec{p}^{T}C\\vec{p}$. We want to maximize this expression. To avoid $\\|\\vec{p}\\|\\to\\infty$ we will only consider unit vectors, i.e. we constrain $\\vec{p}$ to be normalized: $\\vec{p}^T\\vec{p}=1$. Maximize the expression with this constraint (which can be done using a Lagrangian multiplier). Conclude that a suitable $\\vec{p}$ has to be an eigenvector of $C$ and describe which of the eigenvectors is optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c4f39",
   "metadata": {
    "deletable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e80b023dcc7cccd3fb26e1a54161529",
     "grade": true,
     "grade_id": "ex3_a_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82917961",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "38bda87c9767782aa3323595aaf98116",
     "grade": false,
     "grade_id": "ex3_b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Second step:\n",
    "(Outline for the interested reader. You do not have to solve this second part.)\n",
    "\n",
    "In the second step the statement for the general case of an $m$-dimensional projection space is proofed by induction:\n",
    "\n",
    "Assume the statement has been shown for the $(m-1)$-dimensional projection space, spanned by the $m-1$ (orthonormal) eigenvectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$ corresponding to the $(m-1)$ largest eigenvalues $\\lambda_1,\\ldots,\\lambda_{m-1}$. Now find a (unit) vector $\\vec{p}_m$, orthogonal to the existing vectors $\\vec{p}_1,\\ldots,\\vec{p}_{m-1}$, that maximizes the projected variance $\\vec{p}_m^TC\\vec{p}_m$. Proceed similar to the first step, but with additional Lagrangian multipliers to enforce the orthogonality constraint. One can then show that the new vector $\\vec{p}_m$ is an eigenvector of $C$. Finally it can be shown that the variance is maximized for the eigenvector corresponding to the $m$-th largest eigenvalue $\\lambda_m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433f1251",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1edc68066ccb517eff5424b182e4672f",
     "grade": true,
     "grade_id": "cell-e302aa2226cfa4fc",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
