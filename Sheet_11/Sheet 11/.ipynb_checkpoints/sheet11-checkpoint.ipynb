{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9a0e81",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bd1a7235f844ff174a795a6efe33a78",
     "grade": false,
     "grade_id": "h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Osnabr√ºck University - Machine Learning (Summer Term 2021) - Prof. Dr.-Ing. G. Heidemann, Ulf Krumnack, Axel Schaffland"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f48db79",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9e7d36d014ba44abbf2fe8283bbe76c",
     "grade": false,
     "grade_id": "h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Exercise Sheet 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f9997a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08bad10a2f32056ae6765b3398ef2ff6",
     "grade": false,
     "grade_id": "h02",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This week's sheet should be solved and handed in before **2:00pm of Tuesday, July 05, 2021**. If you need help (and Google and other resources were not enough), feel free to contact your groups' designated tutor or whomever of us you run into first. Please upload your results to your group's Stud.IP folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ff864",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4b27cb5dacc2a8f189b4a024a249123",
     "grade": false,
     "grade_id": "ex3_h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 1: Probability Theory [4 Points]\n",
    "\n",
    "Consider three bags filled with three types of candy. The table below indicates for each bag how many candies of each type are in each bag.\n",
    "\n",
    "\n",
    "| contains        | green candy | blue candy | red candy | **total** |\n",
    "| --------------- | ----------- | ---------- | --------- | --------- |\n",
    "| **cyan bag**    |          10 |          4 |         2 |    **16** |\n",
    "| **magenta bag** |           5 |          7 |         2 |    **14** |\n",
    "| **yellow bag**  |           2 |          2 |         8 |    **12** |\n",
    "| **total**       |      **17** |     **13** |    **12** |    **42** |\n",
    "\n",
    "\n",
    "In the following we denote the bags as $B=\\{c,m,y\\}$ and the candies as $C=\\{r, g, b\\}$. So the probability for drawing a blue candy from the cyan bag would be: $P(C=b|B=c)=\\frac{4}{16}=0.25$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bf219b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "684e1af95867313bd4f919ae5fee62f7",
     "grade": false,
     "grade_id": "ex3a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### a)\n",
    "\n",
    "Give a verbal description of the following events and compute their probabilities:\n",
    "$$\n",
    "\\begin{align*}\n",
    "P(C=b|B=m) &= ? \\\\\n",
    "P(C=g|B=y) &= ? \\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720449f4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "330c0839d32a776b512271f4db282aa1",
     "grade": true,
     "grade_id": "ex3a_sol",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Drawing a "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38dba9d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98364b6014133dee9d20258e2e7f2fdf",
     "grade": false,
     "grade_id": "ex3b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b)\n",
    "\n",
    "Now assume that you randomly choose a bag and then randomly draw a candy from that bag. Assume that the probability of drawing a random bag is uniformly distributed. What is the probability that the candy is red?\n",
    "\n",
    "$$P(C=r) = ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d5db65",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "39f7522d88bfd957f8344b068e015a2b",
     "grade": true,
     "grade_id": "ex3b_sol",
     "locked": false,
     "points": 0.5,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462c74fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f862ff4a8402d584f41ca2607fa2ed5",
     "grade": false,
     "grade_id": "ex3c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### c)\n",
    "\n",
    "Someone has choosen a red candy from one of the bags, but does not tell you from which bag it originated.\n",
    "Compute the probability that it was from the yellow bag:\n",
    "    \n",
    "$$P(B=y|C=r) = ?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3081e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3e3ee30a2aab7fe8e88f4dd9fafe3f1",
     "grade": true,
     "grade_id": "ex3c_sol",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdb9b3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ad9872a92f559099447302a576a6cdd1",
     "grade": false,
     "grade_id": "ex3d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### d) \n",
    "\n",
    "Let's assume we draw with the following probabilities from each bag: $P(B=c)=0.2$, $P(B=m)=0.7$, $P(B=y)=0.1$.\n",
    "What are the probabilities to draw a green, blue or red candy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d4b6b1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b3f594e9c3e580e545a616e28d9aa706",
     "grade": true,
     "grade_id": "ex3d_sol",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b9b4a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7084ab3bb6955e10da05542f9b6c9c44",
     "grade": false,
     "grade_id": "ex4_h00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 4: Bayes classifier [4 Points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f58c7d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fb3b657e01fb2d49908d556e67d45098",
     "grade": false,
     "grade_id": "ex4_h01",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Consider the following data set. There are four features, running nose ($N$), coughing ($C$), reddened skin ($R$), and fever ($F$), each of which can take the values true ($+$) or false ($-$).\n",
    "\n",
    "| Diagnosis ID  | $N$ | $C$ | $R$ | $F$ | Classification     |\n",
    "|---------------|-----|-----|-----|-----|--------------------|\n",
    "|     $d_1$     | $+$ | $+$ | $+$ | $-$ | positive (ill)     |\n",
    "|     $d_2$     | $+$ | $+$ | $-$ | $-$ | positive (ill)     |\n",
    "|     $d_3$     | $-$ | $-$ | $+$ | $+$ | positive (ill)     |\n",
    "|     $d_4$     | $+$ | $-$ | $-$ | $-$ | negative (healthy) |\n",
    "|     $d_5$     | $-$ | $-$ | $-$ | $-$ | negative (healthy) |\n",
    "|     $d_6$     | $-$ | $+$ | $+$ | $-$ | negative (healthy) |\n",
    "\n",
    "Solve the following problems either by hand or programmatically. Assume all features are conditionally independent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2515315c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9b2dcf3aeca6725d4c236a445d45178",
     "grade": false,
     "grade_id": "ex4_a00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### a)\n",
    "\n",
    "Determine all probabilities required to apply a Bayes classifier for predicting whether a new person is ill or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72c29a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ae740ad20f4b0272f2e98c71c2e47d2",
     "grade": true,
     "grade_id": "ex4_a_sol",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7a119",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "71f936ae997279863b80844519242c05",
     "grade": false,
     "grade_id": "ex4_b00",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### b)\n",
    "Person $p_1$ is coughing and has fever. Person $p_2$ has a running nose and reddened skin. Person $p_3$ is coughing, suffers from reddened skin and has fever. Determine the probability of being ill for all persons $p1, p2, p3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dedd7ff",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3216eaec57ef317f872741a820b3b422",
     "grade": true,
     "grade_id": "ex4_b_sol",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac268fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbd65ba5b946006b9818762d0427188e",
     "grade": false,
     "grade_id": "cell-rl-th",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Assignment 3: Reinforcement Learning Theory [4 Points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44610611",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "efbbf95ba774832e36bacca726a6f90a",
     "grade": false,
     "grade_id": "cell-rl-th-aq",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### a) Weak teacher\n",
    "\n",
    "Reinforcement learning is often described as being different from both supervised and unsupervised learning by providing a \"weak teacher\". Who is this \"teacher\" and why is she \"weak\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49956384",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6b384f523c205c8c94386fa15d8c6e60",
     "grade": true,
     "grade_id": "cell-rl-th-aa",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a617bf5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eefe866ea4d7bc42c8887a742121c16b",
     "grade": false,
     "grade_id": "cell-rl-th-bq",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### b) Markov decision process\n",
    "\n",
    "Reinforcement learning is usually restricted to first-order Markov decision processes. What does this mean and what are the practical consequences. How would the formulae change when resorting to second-order Markov decision processes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c918ee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "14abbd9b4e749d4ccd0c3c2e3c79565c",
     "grade": true,
     "grade_id": "cell-rl-th-ba",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ccb94e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9f9c1c6623b1dae0073b8f316915a86",
     "grade": false,
     "grade_id": "cell-rl-th-cq",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### c) The Q-function\n",
    "\n",
    "The Q-function can be written as $Q(s,a) = r(s,a) + \\gamma \\operatorname{argmax}_{a'} Q(s,a')$.\n",
    "Explain this function in your own words. What does the Q-value represent? What is the problem with that formula and how is this problem resolved in Q-learning? How would you represent this function when implementing Q-learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d2026",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "babfe21efffd611cd7252578e269842a",
     "grade": true,
     "grade_id": "cell-rl-th-ca",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b9478",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9577c1bdc8868cf051858798346d2c9e",
     "grade": false,
     "grade_id": "cell-rl-th-dq",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### d) Goal state\n",
    "\n",
    "In game playing there often is a goal state (game won/lost), and so is in the maze example from the lecture slides. Discuss the role of this goal state for the Q-Learning algorithm. Describe a learning scenario without a goal state."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095bd65d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fcf6634b11dedab5f8da9fc05871864",
     "grade": true,
     "grade_id": "cell-rl-th-da",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ae7a5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e66ff52dde875ccb0597aff60415444",
     "grade": false,
     "grade_id": "ex-rl",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Assignment 4: Reinforcement Learning [8 Points]\n",
    "\n",
    "In this assignment you will have a look at the Q-Learning algorithm described in the lecture (ML-10 Slide 18). For this we generate a field with random rewards. A learning agent is then exploring the field and learns the optimal path to navigate through it. The code below is again filled with some ``TODO``s that should be filled by you in order to implement the Q-Learning algorithm. \n",
    "\n",
    "Below the code there are some questions! You also find a free-code field for a complete own implementation. You may use your own test mazes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece21112",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "\n",
    "def generate_field(x, y, num_rewards, max_reward):\n",
    "    \"\"\"\n",
    "    Generate a random game field with rewards.\n",
    "    \n",
    "    Args:\n",
    "        x (int):            x dimension of the field\n",
    "        y (int):            y dimension of the field \n",
    "        num_rewards (int):  the number of rewards that should be randomly placed\n",
    "        max_reward (int):   the maximum reward that can be placed \n",
    "        \n",
    "    Returns:\n",
    "        ndarray: A field with randomly initialized rewards, the rest of the \n",
    "        entries is zero\n",
    "    \"\"\"\n",
    "    \n",
    "    # Change or comment out to get different random data in each run\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    field = np.zeros((y,x), dtype=np.uint8)\n",
    "    \n",
    "    for i in range(num_rewards):\n",
    "        field[rand.randint(y), rand.randint(x)] = rand.choice(max_reward)\n",
    "    \n",
    "    return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd1cd7",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5392fc176308b0b1cd2d039240fe0329",
     "grade": true,
     "grade_id": "ex2a_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    \"\"\"\n",
    "    This class contains all the necessary methods to navigate through\n",
    "    a maze or game with the help of a little bit of Q-Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, field, actions, gamma):\n",
    "        \"\"\"\n",
    "        Initializes the QLearning Algorithm with the necessary parameters.\n",
    "        All q values are stored in self.q - this is an array that has\n",
    "        ACTIONS x map_x x map_y dimensions to store a value for each action\n",
    "        in each field. The starting position self.pos is randomly initialized.\n",
    "        \n",
    "        Args:\n",
    "            field (ndarray):  the map\n",
    "            actions (list):   the available actions\n",
    "            gamma (float):    the gamma in the lecture slides\n",
    "        \n",
    "        Returns:\n",
    "            QLearning: An instance that can be used for Q-Learning on the field\n",
    "        \"\"\"\n",
    "        # q stores the q_values for each action in each space of the field.\n",
    "        self.field = field\n",
    "        self.actions = actions\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Remember the map extend for further navigation.\n",
    "        self.map_y = self.field.shape[0]\n",
    "        self.map_x = self.field.shape[1]\n",
    "        \n",
    "        # Create q value matrix.\n",
    "        self.q = np.zeros((len(self.actions), self.map_y, self.map_x))\n",
    "\n",
    "        # Start on a random position in the field.\n",
    "        self.pos = [np.random.randint(self.map_y), np.random.randint(self.map_x)]\n",
    "        self.fig, self.axes = plt.subplots(3, 3, num='QLearning State')\n",
    "        for ax in self.axes.flat:\n",
    "            ax.axis('off')\n",
    "\n",
    "    def get_coordinates(self, position, action):\n",
    "        \"\"\"\n",
    "        Returns the coordinates that follow a certain action, depending\n",
    "        on the current position of the learner. If the border is reached\n",
    "        the agent just stops there.\n",
    "        \n",
    "        Args:\n",
    "            position (pair):  the current position\n",
    "            action (string):  the action that should be performed (one of: 'up', 'down', ...)\n",
    "            \n",
    "        Returns:\n",
    "            pair of int: the updated coordinates\n",
    "        \"\"\"\n",
    "        # return the right new coordinates depending on the position\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Implementation of the update step. Closely follows the Algorithm described on\n",
    "        ML-10 Sl.18. Note that you have attributes available as specified in the\n",
    "        __init__ method of this class, in addition to that is the FIELD variable that\n",
    "        stores the real field the agent is iterating about, as well as ACTIONS which\n",
    "        stores the available actions.\n",
    "        \"\"\"\n",
    "        # Select a random action that should be performed next.\n",
    "        # Be careful to handle the case where you hit the wall!\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Receive the reward for the new position from the field.\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # Update the q-value for the performed action.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Update the position of the player to the new field.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Plots the current state.\n",
    "        \"\"\"\n",
    "        fs = 8\n",
    "        for i, action in enumerate(self.actions):\n",
    "            ax = self.axes.flat[2*i + 1]\n",
    "            ax.cla()\n",
    "            ax.set(title=action)\n",
    "            ax.set_xticks(np.arange(self.q[i,:,:].shape[1]))\n",
    "            ax.set_yticks(np.arange(self.q[i,:,:].shape[0]))\n",
    "            ax.imshow(self.q[i,:,:], interpolation='None')\n",
    "            for j in range(self.q.shape[1]):\n",
    "                for k in range(self.q.shape[2]):\n",
    "                    text = ax.text(k, j, \"{:.1f}\".format(self.q[i,j,k],1),\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontsize=fs)\n",
    "                    plt.setp(text, path_effects=[\n",
    "        PathEffects.withStroke(linewidth=1, foreground=\"w\")])\n",
    "\n",
    "        self.fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce14c53",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1f1dfac9b69ef54485d6fbef2de3ca1",
     "grade": true,
     "grade_id": "ex2b_solution",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "\n",
    "\n",
    "# Determine the size of the field, change this parameters as you like\n",
    "m_x = 5\n",
    "m_y = 4\n",
    "\n",
    "steps = 500\n",
    "\n",
    "actions = ['up','left','right','down']  # Those are the availabe actions for the QLearning.\n",
    "field = generate_field(m_x, m_y, num_rewards=5, max_reward=10) # The field that is used for learning.\n",
    "\n",
    "# Plotting the generated field\n",
    "fs = 18\n",
    "figure, ax = plt.subplots()\n",
    "#plt.axis('off')\n",
    "ax.imshow(field, interpolation='none')\n",
    "ax.set_xticks(np.arange(field.shape[1]))\n",
    "ax.set_yticks(np.arange(field.shape[0]))\n",
    "for j in range(field.shape[0]):\n",
    "    for k in range(field.shape[1]):\n",
    "        text = plt.text(k, j, field[j,k], ha=\"center\", va=\"center\", color=\"black\", fontsize=fs)\n",
    "        plt.setp(text, path_effects=[PathEffects.withStroke(linewidth=3, foreground=\"w\")])\n",
    "\n",
    "figure.suptitle(\"Field\",fontsize=fs)          \n",
    "figure.canvas.draw()\n",
    "\n",
    "\n",
    "# Generate a QLearning instance with the right parameters.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Now we perform steps many learning iterations on the field with\n",
    "# the generated QLearning instance.\n",
    "for i in range(steps):     \n",
    "    player.update()\n",
    "    player.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90467de8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "abb7d97c008b533c8d2649cd3e54f3c6",
     "grade": false,
     "grade_id": "ex2_x",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Explain in your own words, how the algorithm works. What is depicted on the resulting plots. How can an action policy be derived from these data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fb50b4",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7b61f070ffb0153e5dd4767692d90b83",
     "grade": true,
     "grade_id": "ex2c_solution",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f894a09f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ebf1427a0356c02f45ce383edafe01b5",
     "grade": false,
     "grade_id": "ex2_0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You are also free to write your complete own implementation of the QLearning algorithm (instead of completing the code above). Use the following cell for your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cbab65",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3580d7ecdc9edca9b27f5493dde82a0b",
     "grade": true,
     "grade_id": "ex2_alternative",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
